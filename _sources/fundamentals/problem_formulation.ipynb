{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problem formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Data\n",
    "- Model\n",
    "- Loss\n",
    "- Optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inputs\n",
    "\n",
    "- $m$: number of samples and associated results in the dataset.\n",
    "- $n$: number of features. A **feature** is an attribute (a property) of the data samples.\n",
    "- $\\pmb{x}^{(i)}$: data **sample** (example), vector of $n$ features.\n",
    "- $x^{(i)}_j$: value of the $j$th feature for the $i$th data sample.\n",
    "\n",
    "$$\\pmb{x}^{(i)} = \\begin{pmatrix}\n",
    "       \\ x^{(i)}_1 \\\\\n",
    "       \\ x^{(i)}_2 \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ x^{(i)}_n\n",
    "     \\end{pmatrix} \\in \\pmb{R}^n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Design matrix\n",
    "\n",
    "- $\\pmb{X}$: matrix of the form (*samples, features*) expected by most ML algorithms and called *design matrix*.\n",
    "  - First dimension is for the $m$ samples.\n",
    "  - Second dimension is for the $n$ features of each sample.\n",
    "\n",
    "$$\\pmb{X} = \\begin{bmatrix}\n",
    "       \\ x^{(1)T} \\\\\n",
    "       \\ x^{(2)T} \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ x^{(m)T} \\\\\n",
    "     \\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "       \\ x^{(1)}_1 & x^{(1)}_2 & \\cdots & x^{(1)}_n \\\\\n",
    "       \\ x^{(2)}_1 & x^{(2)}_2 & \\cdots & x^{(2)}_n \\\\\n",
    "       \\ \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "       \\ x^{(m)}_1 & x^{(m)}_2 & \\cdots & x^{(m)}_n\n",
    "     \\end{bmatrix} \\in \\pmb{R}^{m \\times n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multidimensional data: reshaping\n",
    "\n",
    "A bitmap image can be represented as a 3D multidimensional array (*height, width, color_channels*).\n",
    "\n",
    "A video can be represented as a 4D multidimensional array (*frames, height, width, color_channels*).\n",
    "\n",
    "They have to be **reshaped**, or *flattened* in that case, into a vector before being fed to most ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Image to vector](images/image2vector.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multidimensional data: scaling\n",
    "\n",
    "Individual pixel values for images and videos are typically integers in the [0:255] range.\n",
    "\n",
    "Scaling them to obtain floats into the $[0,1]$ range is a common practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Targets\n",
    "\n",
    "- $y^{(i)}$: **target**, or **label** (expected result) for a data sample.\n",
    "- $\\pmb{y}$: vector of labels for all $m$ samples in a dataset.\n",
    "\n",
    "$$\\pmb{y} = \\begin{pmatrix}\n",
    "       \\ y^{(1)} \\\\\n",
    "       \\ y^{(2)} \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ y^{(m)}\n",
    "     \\end{pmatrix} \\in \\pmb{R}^m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition\n",
    "\n",
    "The representation learnt from data during training is called a **model**. It defines the relationship between inputs and outputs, and thus produces results from data. Most (but not all) ML systems are model-based.\n",
    "\n",
    "[![Extract from the book Hands-on Machine Learning with Scikit-Learn & TensorFlow by A. GÃ©ron](images/instance_model_learning.png)](https://github.com/ageron/handson-ml2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hypothesis function\n",
    "\n",
    "- $\\pmb{\\theta}$ (sometime noted $\\pmb{\\omega}$): model's internal parameters vector.\n",
    "- $h_\\theta()$: model's prediction function (*hypothesis function*), using the model parameters $\\pmb{\\theta}$ to define the relationship between features and labels.\n",
    "- $y'^{(i)}$ (sometimes noted $\\hat{y}^{(i)}$): hypothesis function output (model prediction).\n",
    "\n",
    "$$y'^{(i)} = h_\\theta(\\pmb{x}^{(i)})$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multiclass classification\n",
    "\n",
    "* $\\pmb{y}^{(i)}$ et $\\pmb{y}'^{(i)}$ are vectors with as many elements as the number of predicted classes $K$.\n",
    "* $\\pmb{y}^{(i)}$ (the *ground truth*) is a **binary vector** of $K$ values. $y^{(i)}_k$ is equal to 1 if the $i$th sample's class corresponds to $k$, 0 otherwise.\n",
    "* $\\pmb{y}'^{(i)}$ is a **probability vector** of $K$ values, computed by the model. $y'^{(i)}_k$ represents the probability that the $i$th sample belongs to class $k$.\n",
    "\n",
    "$$\\pmb{y}^{(i)} = \\begin{pmatrix}\n",
    "       \\ y^{(i)}_1 \\\\\n",
    "       \\ y^{(i)}_2 \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ y^{(i)}_K\n",
    "     \\end{pmatrix} \\in \\pmb{R}^K\n",
    "\\;\\;\\;\n",
    "\\pmb{y}'^{(i)} = \\begin{pmatrix}\n",
    "       \\ y'^{(i)}_1 \\\\\n",
    "       \\ y'^{(i)}_2 \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ y'^{(i)}_K\n",
    "     \\end{pmatrix} \\in \\pmb{R}^K$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model lifecycle\n",
    "\n",
    "There are two (repeatable) phases:\n",
    "\n",
    "- **Training**: using training input samples, the model learns to find a relationship between features and labels.\n",
    "- **Inference**: the trained model is used to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hyperparameters\n",
    "\n",
    "- Many ML models also have user-defined properties called **hyperparameters**.\n",
    "  - Examples: maximum depth of a decision tree, number oy layers of a neural network...\n",
    "- Contrary to internal parameters, they are not automatically updated during training.\n",
    "- The hyperparameters directly affect the model's performance and must be tweaked during the training and tuning steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition\n",
    "\n",
    "- $\\mathcal{L_{\\pmb{X, y}}(\\pmb{\\theta})}$, sometimes noted $\\mathcal{J_{\\pmb{X, y}}(\\pmb{\\theta})}$: **loss** (or **cost**) function that quantifies the difference, often called **error**, between expected results (called *ground truth*) and actual results computed by the model.\n",
    "- During model training, the input dataset $\\pmb{X}$ and the expected results $\\pmb{y}$ are treated as constants. The loss depends solely on the model parameters $\\pmb{\\theta}$. To simplify notations, the loss function will be written $\\mathcal{L(\\pmb{\\theta})}$.\n",
    "- Different loss functions exist. The choice depends on the learning type. See [Losses](./losses) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definition\n",
    "\n",
    "- Used during the training phase.\n",
    "- Objective: find the set of model parameters $\\pmb{\\theta}^{*}$ that minimizes the loss.\n",
    "- For each learning type, several algorithms of various complexity exist.\n",
    "\n",
    "[![Optimization: linear regression](images/LossSideBySide.png)](https://developers.google.com/machine-learning/crash-course/reducing-loss/an-iterative-approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
