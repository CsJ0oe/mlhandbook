{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- Neural Style Transfer\n",
    "- Generative Adversarial Networks (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural Style Transfer in a nutshell\n",
    "\n",
    "- Reproduce an image with a new artistic style provided by another image.\n",
    "- Blend a *content* image and a *style reference* image in a stylized output image.\n",
    "- First described in [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576) by Gatys et al (2015). Many refinements and variations since."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example (Prisma app)\n",
    "\n",
    "[![Prisma style transfer example](images/style_transfer_prisma.png)](https://harishnarayanan.org/writing/artistic-style-transfer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Underlying idea: loss minimization\n",
    "\n",
    "![Content loss](images/content-loss.png)\n",
    "![Style loss](images/style-loss.png)\n",
    "![Total loss](images/total-loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The content loss\n",
    "\n",
    "- Content = high-level structure of an image.\n",
    "- Can be captured by the upper layer of a convolutional neural network.\n",
    "- Content loss for a layer = distance between the feature maps of the content and generated images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The style loss\n",
    "\n",
    "- Style = low-level features of an image (textures, colors, visual patterns).\n",
    "- Can be captured by using correlations across the different feature maps (filter responses) of a convnet.\n",
    "- Feature correlations are computed via a Gram matrix (outer product of the feature maps for a given layer).\n",
    "- Style loss for a layer = distance between the Gram matrices of the feature maps for the style and generated images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The total variation loss\n",
    "\n",
    "- Sum of the absolute differences for neighboring pixel-values in an image. Measures how much noise is in the image.\n",
    "- Encourage spatial continuity in the generated image (denoising).\n",
    "- Act as a regularization loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent\n",
    "\n",
    "- Objective: minimize the total loss.\n",
    "- Optimizer: [L-BFGS](http://aria42.com/blog/2014/12/understanding-lbfgs) (original choice made by Gatys et al.) or Adam.\n",
    "\n",
    "![Animation of style transfer](images/style_transfer_animated.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Adversarial Networks (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GAN in a nutshell\n",
    "\n",
    "- Simultaneously train two models:\n",
    "  - One tries to generate realistic data.\n",
    "  - The other tries to discriminate between real and generated data.\n",
    "- Each model is trained to best the other.\n",
    "- First described in [Generative Adversarial Nets\n",
    "](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) by Goodfellow et al. (2014).\n",
    "- [NIPS 2016 Tutorial](https://arxiv.org/abs/1701.00160)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[![GAN overview](images/gan1.png)](https://www.tensorflow.org/tutorials/generative/dcgan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[![GAN process](images/gan2.png)](https://www.tensorflow.org/tutorials/generative/dcgan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training process\n",
    "\n",
    "- The generator creates images from random noise.\n",
    "- Generated images are mixed with real ones.\n",
    "- The discriminator is trained on these mixed images.\n",
    "- The generator's parameters are updated in a direction that makes the discriminator more likely to classify generated data as \"real\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: generate handwritten digits\n",
    "\n",
    "This example trains a Deep Convolutional GAN on the MNIST dataset. Check it out on the [TensorFlow website](https://www.tensorflow.org/tutorials/generative/dcgan)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Specificities and gotchas\n",
    "\n",
    "- A GAN is a dynamic system that evolves at each training step.\n",
    "- Interestingly, the generator never sees images froms the training set directly: all its informations come from the discriminator.\n",
    "- Training can be tricky: noisy generated data, vanishing gradients, domination of one side...\n",
    "- GAN convergence theory is an active area of research.\n",
    "- [GAN Open Questions](https://distill.pub/2019/gan-open-problems/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GAN progress on face generation\n",
    "\n",
    "[![GAN progress from 2014 to 2018](images/gan_2014_2018.jpg)](https://twitter.com/goodfellow_ian/status/1084973596236144640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The GAN landscape\n",
    "\n",
    "[![GAN flavours](images/gan_flavours.png)](https://blog.floydhub.com/gans-story-so-far/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some GAN flavours\n",
    "\n",
    "- [DCGAN](https://arxiv.org/abs/1511.06434) (2016): use deep convolutional networks for generator and discriminator.\n",
    "- [CycleGAN](https://arxiv.org/abs/1703.10593v6) (2017): image-to-image translation in the absence of any paired training examples.\n",
    "- [StyleGAN](https://arxiv.org/abs/1812.04948) (2019): fine control of output images.\n",
    "- [GAN - The Story So Far](https://blog.floydhub.com/gans-story-so-far/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GAN use cases: not just images!\n",
    "\n",
    "- Writing a novel \"in the style of an author\".\n",
    "- [Generating music](https://arxiv.org/abs/1805.07848) ([samples](https://www.youtube.com/watch?v=vdxCqNWTpUs)).\n",
    "- Generating realistic passwords for hackers.\n",
    "- Generating videos ([example](https://www.youtube.com/watch?time_continue=3&v=ab64TWzWn40&feature=emb_logo)).\n",
    "- [Generating video game levels](https://arxiv.org/abs/1910.01603).\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
