{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Overview\n",
    "- Gradient descent types\n",
    "- Model parameters update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An iterative approach\n",
    "\n",
    "The model's parameters are iteratively updated until an optimum is reached.\n",
    "\n",
    "[![Iterative approach](images/GradientDescentDiagram.png)](https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The gradient descent algorithm\n",
    "\n",
    "- Used in several ML models, including neural networks.\n",
    "- General idea: converging to a loss function's minimum by updating model parameters in small steps, in the **opposite direction** of the loss function **gradient**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The notion of gradient\n",
    "\n",
    "- Expresses the variation of a function relative to the variation of its parameters.\n",
    "- Vector containing partial derivatives of the function *w.r.t.* each of its $P$ parameters.\n",
    "\n",
    "$$\\nabla_{\\theta}\\mathcal{L}(\\boldsymbol{\\pmb{\\theta}}) = \\begin{pmatrix}\n",
    "       \\ \\frac{\\partial}{\\partial \\theta_1} \\mathcal{L}(\\boldsymbol{\\theta}) \\\\\n",
    "       \\ \\frac{\\partial}{\\partial \\theta_2} \\mathcal{L}(\\boldsymbol{\\theta}) \\\\\n",
    "       \\ \\vdots \\\\\n",
    "       \\ \\frac{\\partial}{\\partial \\theta_P} \\mathcal{L}(\\boldsymbol{\\theta})\n",
    "     \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1D gradient descent (one parameter)\n",
    "\n",
    "![Gradient Descent](images/gradient_descent_1parameter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2D gradient (two parameters)\n",
    "\n",
    "![Tangent Space](images/tangent_space.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2D gradient descent\n",
    "\n",
    "![Gradient Descent](images/gradient_descent_2parameters.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient descent types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Batch Gradient Descent\n",
    "\n",
    "The gradient is computed on the whole dataset before model parameters are updated.\n",
    "\n",
    "- Advantages: simple and safe (always converges in the right direction).\n",
    "- Drawback: can become slow and even untractable with a big dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "The gradient is computed on only one randomly chosen sample whole dataset before parameters are updated.\n",
    "\n",
    "- Advantages:\n",
    "  - Very fast.\n",
    "  - Enables learning from each new sample (*online learning*).\n",
    "- Drawback:\n",
    "  - Convergence is not guaranteed.\n",
    "  - No vectorization of computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mini-Batch SGD\n",
    "\n",
    "The gradient is computed on a small set of samples, called a *batch*, before parameters are updated.\n",
    "\n",
    "- Combines the advantages of batch and stochastic GD.\n",
    "- Default method for many ML libraries.\n",
    "- The mini-batch size varies between 10 and 1000 samples, depending of the dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model parameters update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning rate\n",
    "\n",
    "$\\eta$ is the update factor for parameters once gradient is computed, called the **_learning rate_**.\n",
    "\n",
    "It has a direct impact on the \"speed\" of the gradient descent.\n",
    "\n",
    "$$\\pmb{\\theta_{next}} = \\pmb{\\theta} - \\eta\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}(\\boldsymbol{\\theta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Importance of learning rate\n",
    "\n",
    "![Learning rate](images/learning_rate.png)\n",
    "\n",
    "[Interactive exercise](https://developers.google.com/machine-learning/crash-course/fitter/graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The local minima problem\n",
    "\n",
    "![Local minima](images/local_minima.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Gradient Descent](images/gd_ng.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
